# What did you build?
I built an interative style quiz that lets a user take a quiz to determine which hair salon service would best fit their needs. The quiz is simple, with it displaying 5 questions that the user can answer. At the end of the quiz, the user is shown the style that matches their results, and it recommends a hair salon service to fit that need. The code works by creating a Javascript file that stores the quiz data into an object. The quiz results are also stored in a seperate object, both of which are referenced in the quiz functions. Using a function, the object's key is selected based on the results, and the answer is pushed into the HTML through the Document Object Model (DOM). Event listeners are looking out for interactions of each question and update based on the answer chosen. Because I am well versed in Javascript, I did not have a hard time understanding what Claude was outputting when this feature was being built, and it made the process of having a functional quiz very quickly.

# How did micro-iteration feel?
The micro-iteration felt like a cycle that made it easier to create a feature. By breaking it into chunks, I felt that I had more control of what I was working on since it was more managable, and the code would update in chunks as well which made it easy to test that things were working properly on my end. Working in small steps felt natural because whenever I have used AI to build parts of a project, I always work in small steps because it would never get the exact details correctly when done all together in one go.

# What did self-review catch?
Self-review mostly caught bugs and edge cases where something may interfere with a future step's development. For bugs, it caught those that would interfere mostly with the CSS, while for edge cases it thought ahead as to what it was going to implement and worked around it to make sure that nothing collides in the codebase for the actual functionality. One example of Claude self-reviewing itself for CSS was when it realized that a descendant selector was unintentionally styling the result screen's heading and overriding its font size due to higher selector specificity. It corrected this by changing the selector to a direct-child combinator (>) and updating the matching media query rule to properly scope the styles.

# Tool impressions.
The tool was very interesting since it integrates directly with GitHub and automatically pushes the code onto its own branch. I personally have only used branches once and never really understood how branches and pull requests work, but after working with the tool I figured out its use when working with others. Claude essentially functioned as a collaborator that assisted me, which would've worked the same on GitHub for pull requests if another human was the person on the other end. This made it very easy to learn GitHub itself, as well as easily communicating with the AI. Overall, I liked the direct integration with the repository, since it made it easier to push changes immediately to the website. One thing I didn't particularly like though was that I had to keep merging the branch and pulling the files onto my local machine to view the changes. It felt like more work to do.

# Self-review patterns.
The AI consistently caught CSS issues and edge cases when creating the feature. Most CSS issues were due to changes in the quiz styling that interfered with prexisting code, but it was easily resolved after a few steps. As for edge cases, Claude would think ahead and find the exact code it was planning to use. It would also consider how the code might fail if a certain action was taken, which made it important to know that these potential bugs could break functionality. The only issue it missed that I caught for myself was that the content wasn't centered to the middle of the page and was slightly misbalanced to the left. It was a quick fix, but Claude did not catch it.

# Browser tool vs. CLI comparison.
The browser experience compared positively, and I would say that it is a great tool with a different purpose of use. The browser tool's integration with GitHub made it much easier to push the new code onto a repository automatically. It also immediately read all of the project files without taking a long time. However, I didn't like that I had to continue merging branches to view changes and pull them locally. It would be easier to simply push everything at the end and view the changes gradually locally, which is how the CLI version does it. I personally would still prefer to use Claude Code CLI because of the fact that I can easily view my files locally without needing to pull the latest changes from GitHub. Claude Code CLI also has the ability to change files on my computer if it requries something to be installed, which is another benefit. The downside for the CLI version is that it takes more time to set up and it needs more time to think about the file changes from what I experienced. However, I still think the CLI tool is the stronger version.

# When would you use micro-iteration + self-review?
Micro-iteration makes sense for bigger projects that require smaller features to be implemented one at a time. The reason being if this was done all in one prompt, then it may come out with more bugs and not function the way that I expected it to. By working in smaller steps, it makes it easier to control what is being changed before it's too far into the development. I would skip using micro-iteration on features that are too small or not worth it, such as HTML boilerplate or a small addition like adding a paragraph into a section.
